searchState.loadedDescShard("torch_rs", 0, "torch-rs\n计算预测值和目标值之间的均方误差（Mean …\nReLU激活函数。\n神经网络模块通用trait。\n切换到评估模式\n前向传播\n获取所有可训练参数\n切换到训练模式\n线性层（全连接层），实现 y = xW + b。\n偏置参数，形状为 (out_features,)\n切换到评估模式\n前向传播，x @ w + b\nReturns the argument unchanged.\n输入特征数\nCalls <code>U::from(self)</code>.\n创建一个线性层\n输出特征数\n获取所有可训练参数\n切换到训练模式\n是否处于训练模式\n权重参数，形状为 (in_features, out_features)\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\n反向传播（返回输入梯度）\n前向传播\n前向传播：计算输入张量的加法结果\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nComputes the gradient of the mean operation.\nComputes the mean of a tensor.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nFunctional interface for the mean operation.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\n优化器通用trait。\n执行一步参数更新\n清零所有参数的梯度\n随机梯度下降（SGD）优化器。\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\n学习率\n创建SGD优化器\n需要优化的参数列表\n设置学习率\n执行一步参数更新\n清零所有参数的梯度\n张量类型，自动微分的核心对象。\n…\n添加父节点\n…\n创建该张量的操作\n获取数据副本\n张量的实际数据\n返回不带梯度的新张量\n获取张量维度数\nReturns the argument unchanged.\nReturns the argument unchanged.\n梯度\n按索引取子张量\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\n是否为叶子节点\n获取标量值\nComputes the mean of the tensor.\n创建新的张量数据\n用数据创建新张量\n获取元素总数\n创建全1张量\n创建与自身形状相同的全1张量\n父节点（依赖的张量）\n创建与自身形状相同的随机张量\n创建正态分布随机张量\n设置是否需要梯度（链式调用）\n设置是否需要梯度\n是否需要计算梯度\n重塑形状\n设置创建该张量的操作\n获取张量形状\n获取指定维度的长度\n挤压指定或所有为1的维度\n沿第0维堆叠张量\n在指定维度插入新轴\n视图变换\n创建全0张量\n创建与自身形状相同的全0张量\nDataLoader，支持 batch、shuffle、迭代。\n数据集 trait，类似 PyTorch 的 Dataset。\n一个简单的张量数据集实现。\nbatch 大小\n数据张量列表\n数据集引用\nReturns the argument unchanged.\nReturns the argument unchanged.\n获取指定索引的数据和目标\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\n返回数据集长度\n创建新的张量数据集\n创建新的 DataLoader\n获取下一个 batch\n重置迭代器，重新洗牌\n目标张量列表")